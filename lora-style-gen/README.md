# ğŸŒ¸ å°çº¢ä¹¦é£æ ¼æ–‡æ¡ˆç”Ÿæˆå™¨ - LoRA å¾®è°ƒé¡¹ç›®

åŸºäº Qwen2-0.5B-Instruct æ¨¡å‹ï¼Œä½¿ç”¨ LoRA æŠ€æœ¯å¾®è°ƒç”Ÿæˆå°çº¢ä¹¦é£æ ¼çš„æ–‡æ¡ˆã€‚

## ğŸ“‹ é¡¹ç›®ç®€ä»‹

æœ¬é¡¹ç›®é€šè¿‡ LoRA (Low-Rank Adaptation) æŠ€æœ¯å¯¹ Qwen2-0.5B-Instruct æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œä½¿å…¶èƒ½å¤Ÿç”Ÿæˆç¬¦åˆå°çº¢ä¹¦å¹³å°é£æ ¼çš„æ–‡æ¡ˆã€‚é¡¹ç›®åŒ…å«å®Œæ•´çš„è®­ç»ƒã€æ¨ç†ã€è¯„ä¼°å’Œ Web ç•Œé¢åŠŸèƒ½ã€‚

## âœ¨ ä¸»è¦ç‰¹æ€§

- ğŸ¯ **LoRA å¾®è°ƒ**ï¼šä½¿ç”¨ PEFT åº“å®ç°é«˜æ•ˆçš„å‚æ•°é«˜æ•ˆå¾®è°ƒ
- ğŸ“ **é£æ ¼ç”Ÿæˆ**ï¼šç”Ÿæˆç¬¦åˆå°çº¢ä¹¦é£æ ¼çš„æ–‡æ¡ˆï¼ˆåŒ…å« emojiã€æ„Ÿå¹è¯ã€æƒ…æ„Ÿè¡¨è¾¾ç­‰ï¼‰
- ğŸ” **æ•ˆæœå¯¹æ¯”**ï¼šæä¾›åŸºç¡€æ¨¡å‹ä¸å¾®è°ƒæ¨¡å‹çš„è¯¦ç»†å¯¹æ¯”åˆ†æ
- ğŸŒ **Web ç•Œé¢**ï¼šåŸºäº Gradio çš„äº¤äº’å¼ Web åº”ç”¨
- ğŸ“Š **é£æ ¼è¯„ä¼°**ï¼šè‡ªåŠ¨è¯„ä¼°ç”Ÿæˆæ–‡æ¡ˆçš„å°çº¢ä¹¦é£æ ¼å¾—åˆ†

## ğŸ› ï¸ ç¯å¢ƒè¦æ±‚

- Python 3.8+
- PyTorch 2.0+
- CUDAï¼ˆæ¨èï¼Œç”¨äº GPU åŠ é€Ÿï¼‰

## ğŸ“¦ å®‰è£…ä¾èµ–

```bash
pip install torch transformers peft accelerate gradio
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
lora-style-gen/
â”œâ”€â”€ train_lora.py          # LoRA è®­ç»ƒè„šæœ¬
â”œâ”€â”€ inference_lora.py      # æ¨ç†è„šæœ¬ï¼ˆäº¤äº’å¼ï¼‰
â”œâ”€â”€ inference.py           # æ¨ç†è„šæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼‰
â”œâ”€â”€ gradio_app.py          # Gradio Web ç•Œé¢
â”œâ”€â”€ compare_lora.py        # æ¨¡å‹æ•ˆæœå¯¹æ¯”è„šæœ¬
â”œâ”€â”€ train.json             # è®­ç»ƒæ•°æ®ï¼ˆJSON æ ¼å¼ï¼‰
â”œâ”€â”€ xiaohongshu_200.jsonl  # è®­ç»ƒæ•°æ®ï¼ˆJSONL æ ¼å¼ï¼Œ200 æ¡ï¼‰
â”œâ”€â”€ test_prompts.json      # æµ‹è¯•ç”¨ä¾‹
â””â”€â”€ output_lora/           # LoRA æ¨¡å‹è¾“å‡ºç›®å½•
    â”œâ”€â”€ adapter_config.json
    â”œâ”€â”€ adapter_model.safetensors
    â””â”€â”€ checkpoint-*/      # è®­ç»ƒæ£€æŸ¥ç‚¹
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å‡†å¤‡è®­ç»ƒæ•°æ®

è®­ç»ƒæ•°æ®æ ¼å¼ä¸º JSONLï¼Œæ¯è¡ŒåŒ…å«ï¼š
```json
{
  "instruction": "è¯·æ ¹æ®å•†å“ç‰¹å¾å†™ä¸€ä¸ªå°çº¢ä¹¦é£æ ¼çš„æ–‡æ¡ˆï¼š",
  "input": "è“ç‰™è€³æœºï¼Œç»­èˆªé•¿",
  "output": "çœŸçš„ç»äº†ï¼è¿™ä¸ªè“ç‰™è€³æœºç»­èˆªè¶…é•¿âœ¨ å…¥æ‰‹ä¸äºï½"
}
```

### 2. è®­ç»ƒ LoRA æ¨¡å‹

```bash
python train_lora.py
```

è®­ç»ƒå‚æ•°ï¼ˆå¯åœ¨ `train_lora.py` ä¸­ä¿®æ”¹ï¼‰ï¼š
- **åŸºç¡€æ¨¡å‹**ï¼š`Qwen/Qwen2-0.5B-Instruct`
- **LoRA é…ç½®**ï¼š
  - `r=6`ï¼šLoRA ç§©
  - `lora_alpha=8`ï¼šLoRA ç¼©æ”¾å‚æ•°
  - `target_modules=["q_proj", "v_proj"]`ï¼šç›®æ ‡æ¨¡å—
- **è®­ç»ƒå‚æ•°**ï¼š
  - `max_steps=200`ï¼šè®­ç»ƒæ­¥æ•°
  - `learning_rate=4e-5`ï¼šå­¦ä¹ ç‡
  - `per_device_train_batch_size=1`ï¼šæ‰¹æ¬¡å¤§å°
  - `gradient_accumulation_steps=4`ï¼šæ¢¯åº¦ç´¯ç§¯æ­¥æ•°

è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹å°†ä¿å­˜åˆ° `output_lora/` ç›®å½•ã€‚

### 3. ä½¿ç”¨æ¨¡å‹ç”Ÿæˆæ–‡æ¡ˆ

#### æ–¹å¼ä¸€ï¼šäº¤äº’å¼æ¨ç†

```bash
python inference_lora.py
```

#### æ–¹å¼äºŒï¼šä½¿ç”¨ Gradio Web ç•Œé¢

```bash
python gradio_app.py
```

ç„¶ååœ¨æµè§ˆå™¨ä¸­è®¿é—® `http://localhost:7860`

#### æ–¹å¼ä¸‰ï¼šåœ¨ä»£ç ä¸­ä½¿ç”¨

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch

BASE_MODEL = "Qwen/Qwen2-0.5B-Instruct"
LORA_PATH = "./output_lora"

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float16,
    device_map="auto"
)
model = PeftModel.from_pretrained(model, LORA_PATH)
model.eval()

# ç”Ÿæˆæ–‡æ¡ˆ
features = "è“ç‰™è€³æœºï¼Œç»­èˆªé•¿"
prompt = f"è¯·æ ¹æ®å•†å“ç‰¹å¾å†™ä¸€ä¸ªå°çº¢ä¹¦é£æ ¼çš„æ–‡æ¡ˆï¼š\nå•†å“ï¼š{features}\næ–‡æ¡ˆï¼š"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    outputs = model.generate(
        **inputs,
        max_new_tokens=180,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )

result = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(result)
```

### 4. å¯¹æ¯”æ¨¡å‹æ•ˆæœ

#### æ‰¹é‡å¯¹æ¯”æµ‹è¯•

```bash
python compare_lora.py 5  # æµ‹è¯• 5 ä¸ªç”¨ä¾‹
```

#### äº¤äº’å¼å¯¹æ¯”

```bash
python compare_lora.py --interactive
```

å¯¹æ¯”è„šæœ¬ä¼šè¯„ä¼°ï¼š
- ğŸ“Š å°çº¢ä¹¦é£æ ¼è¯„åˆ†ï¼ˆ0-100 åˆ†ï¼‰
- âœ¨ Emoji ä½¿ç”¨æƒ…å†µ
- ğŸ’¬ æ„Ÿå¹è¯å’Œæƒ…æ„Ÿè¯ä½¿ç”¨
- ğŸ“ æ–‡æ¡ˆé•¿åº¦
- ğŸ“š ä¸è®­ç»ƒæ•°æ®çš„ç›¸ä¼¼åº¦

## ğŸ“Š æ¨¡å‹é…ç½®

### LoRA å‚æ•°

å½“å‰é…ç½®ï¼ˆå¯åœ¨ `train_lora.py` ä¸­ä¿®æ”¹ï¼‰ï¼š

```python
LoraConfig(
    r=6,                    # LoRA ç§©ï¼Œæ§åˆ¶å‚æ•°é‡
    lora_alpha=8,          # LoRA ç¼©æ”¾å‚æ•°
    target_modules=["q_proj", "v_proj"],  # ç›®æ ‡æ³¨æ„åŠ›æ¨¡å—
    lora_dropout=0.05,      # Dropout ç‡
    bias="none",            # ä¸è®­ç»ƒåç½®
    task_type="CAUSAL_LM",  # å› æœè¯­è¨€æ¨¡å‹
)
```

### è®­ç»ƒå‚æ•°

```python
TrainingArguments(
    output_dir="output_lora",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=4,
    warmup_steps=10,
    max_steps=200,
    learning_rate=4e-5,
    fp16=True,
    logging_steps=10,
    save_steps=100,
    save_total_limit=2,
)
```

## ğŸ¨ å°çº¢ä¹¦é£æ ¼ç‰¹å¾

æ¨¡å‹å­¦ä¹ çš„å°çº¢ä¹¦é£æ ¼ç‰¹å¾åŒ…æ‹¬ï¼š

- **Emoji ä½¿ç”¨**ï¼šğŸ”¥ âœ¨ ğŸ’— ğŸ’ ç­‰è¡¨æƒ…ç¬¦å·
- **æ„Ÿå¹è¯**ï¼šçœŸçš„ã€å¤ªã€è¶…ã€ç»äº†ã€çˆ±äº†ã€å¿…å…¥ç­‰
- **æƒ…æ„Ÿè¡¨è¾¾**ï¼šæ²»æ„ˆã€å¹¸ç¦ã€èˆ’æœã€ä¸Šå¤´ç­‰
- **ç»“å°¾æ ‡è®°**ï¼šï½ã€ï¼ã€ï¼ï½ ç­‰
- **å£è¯­åŒ–è¡¨è¾¾**ï¼šå…¥æ‰‹ã€ä¸äºã€æ•‘æ˜Ÿã€ç¥å™¨ç­‰
- **æ–‡æ¡ˆé•¿åº¦**ï¼šé€šå¸¸åœ¨ 25-60 å­—ç¬¦ä¹‹é—´

## ğŸ“ˆ æ•ˆæœè¯„ä¼°

`compare_lora.py` è„šæœ¬æä¾›äº†è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡ï¼š

1. **é£æ ¼è¯„åˆ†**ï¼šåŸºäºå¤šä¸ªç»´åº¦è®¡ç®— 0-100 åˆ†çš„é£æ ¼å¾—åˆ†
2. **ç‰¹å¾è¦†ç›–ç‡**ï¼šç»Ÿè®¡å„ç§é£æ ¼ç‰¹å¾çš„å‡ºç°é¢‘ç‡
3. **ä¸è®­ç»ƒæ•°æ®ç›¸ä¼¼åº¦**ï¼šè®¡ç®—ç”Ÿæˆæ–‡æœ¬ä¸è®­ç»ƒæ ·æœ¬çš„ç›¸ä¼¼åº¦
4. **ç»¼åˆè¯„ä¼°**ï¼šç»™å‡ºå¾®è°ƒæ•ˆæœçš„æ€»ä½“è¯„ä»·å’Œæ”¹è¿›å»ºè®®

## ğŸ”§ è‡ªå®šä¹‰é…ç½®

### ä¿®æ”¹åŸºç¡€æ¨¡å‹

åœ¨ `train_lora.py` ä¸­ä¿®æ”¹ï¼š

```python
base_model = "Qwen/Qwen2-0.5B-Instruct"  # æ”¹ä¸ºå…¶ä»–æ¨¡å‹
```

### è°ƒæ•´ LoRA å‚æ•°

å¢åŠ  `r` å€¼å¯ä»¥æå‡æ¨¡å‹å®¹é‡ï¼Œä½†ä¼šå¢åŠ å‚æ•°é‡å’Œè®­ç»ƒæ—¶é—´ï¼š

```python
lora_config = LoraConfig(
    r=16,  # ä» 6 å¢åŠ åˆ° 16
    lora_alpha=32,  # é€šå¸¸è®¾ä¸º r çš„ 2 å€
    # ...
)
```

### ä¿®æ”¹è®­ç»ƒæ•°æ®

1. å‡†å¤‡ JSONL æ ¼å¼çš„æ•°æ®æ–‡ä»¶
2. åœ¨ `train_lora.py` ä¸­ä¿®æ”¹ `train_file` è·¯å¾„
3. æ ¹æ®éœ€è¦è°ƒæ•´ `max_steps` ç­‰è®­ç»ƒå‚æ•°

## ğŸ“ æ•°æ®æ ¼å¼

### è®­ç»ƒæ•°æ®æ ¼å¼ï¼ˆJSONLï¼‰

æ¯è¡Œä¸€ä¸ª JSON å¯¹è±¡ï¼š

```json
{"instruction": "è¯·æ ¹æ®å•†å“ç‰¹å¾å†™ä¸€ä¸ªå°çº¢ä¹¦é£æ ¼çš„æ–‡æ¡ˆï¼š", "input": "å•†å“ç‰¹å¾æè¿°", "output": "å°çº¢ä¹¦é£æ ¼æ–‡æ¡ˆ"}
```

### æµ‹è¯•æ•°æ®æ ¼å¼ï¼ˆJSONï¼‰

```json
[
  "å•†å“ç‰¹å¾1",
  "å•†å“ç‰¹å¾2",
  ...
]
```

## ğŸ› å¸¸è§é—®é¢˜

### 1. å†…å­˜ä¸è¶³

- å‡å° `per_device_train_batch_size`
- å¢åŠ  `gradient_accumulation_steps` ä¿æŒæœ‰æ•ˆæ‰¹æ¬¡å¤§å°
- ä½¿ç”¨ `fp16=True` å¯ç”¨æ··åˆç²¾åº¦è®­ç»ƒ

### 2. ç”Ÿæˆæ•ˆæœä¸ä½³

- å¢åŠ è®­ç»ƒæ•°æ®é‡ï¼ˆå»ºè®®è‡³å°‘ 100+ æ¡ï¼‰
- å¢åŠ è®­ç»ƒæ­¥æ•°ï¼ˆ`max_steps`ï¼‰
- è°ƒæ•´ LoRA å‚æ•°ï¼ˆå¢åŠ  `r` å€¼ï¼‰
- æ£€æŸ¥è®­ç»ƒæ•°æ®è´¨é‡

### 3. æ¨¡å‹åŠ è½½å¤±è´¥

- ç¡®ä¿ `output_lora/` ç›®å½•å­˜åœ¨ä¸”åŒ…å« `adapter_config.json` å’Œ `adapter_model.safetensors`
- æ£€æŸ¥åŸºç¡€æ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®

## ğŸ“š ç›¸å…³èµ„æº

- [PEFT æ–‡æ¡£](https://huggingface.co/docs/peft)
- [Transformers æ–‡æ¡£](https://huggingface.co/docs/transformers)
- [Qwen æ¨¡å‹](https://huggingface.co/Qwen)

## ğŸ“„ è®¸å¯è¯

è¯·å‚è€ƒåŸºç¡€æ¨¡å‹ Qwen2-0.5B-Instruct çš„è®¸å¯è¯ã€‚

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“§ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ Issue åé¦ˆã€‚

