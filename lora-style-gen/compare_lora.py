import json
import re
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
from collections import Counter

BASE_MODEL = "Qwen/Qwen2-0.5B-Instruct"
LORA_PATH = "./output_lora"

# å°çº¢ä¹¦é£æ ¼ç‰¹å¾å…³é”®è¯
XHS_STYLE_KEYWORDS = {
    "emoji": ["ğŸ”¥", "âœ¨", "ğŸ’—", "ğŸ’", "ğŸ›", "â„ï¸", "ğŸ¤–", "ğŸ˜±", "â˜€ï¸", "ğŸ’„", "ğŸ‘ï¸", "ğŸ¼", 
              "ğŸ‘¶", "ğŸ¥›", "ğŸ¥œ", "ğŸƒ", "ğŸ§³", "ğŸŒ¸", "ğŸ©´", "ğŸ›", "ğŸ’â€â™€ï¸", "ğŸ’…", "ğŸ˜´", 
              "ğŸ’§", "ğŸ±", "ğŸ¶", "â›º", "ğŸŒ§ï¸", "âŒ¨ï¸", "ğŸ’»", "ğŸ’", "ğŸ’¤", "ğŸ¦·", "ğŸŒ¬", 
              "ğŸŸ", "ğŸª‘", "ğŸ«§", "ğŸƒâ€â™‚ï¸", "ğŸ§˜â€â™€ï¸"],
    "exclamations": ["çœŸçš„", "å¤ª", "è¶…", "ç»äº†", "çˆ±äº†", "å¿…å…¥", "ä¸äº", "æ•‘æ˜Ÿ", "ç¥å™¨", 
                     "å¤ªç»äº†", "çœŸçš„çˆ±äº†", "çœŸçš„ç»", "å¤ªå¹¸ç¦", "å¤ªèˆ’æœ", "å¤ªæ–¹ä¾¿"],
    "emotional_words": ["æ²»æ„ˆ", "å¹¸ç¦", "èˆ’æœ", "çˆ±äº†", "ç»äº†", "ä¸Šå¤´", "æ‹‰æ»¡", "å¿…å›¤"],
    "ending_markers": ["ï½", "ï¼", "ï¼ï½", "ï½ï¼"],
    "oral_expressions": ["å…¥æ‰‹", "å¿…å…¥", "ä¸äº", "æ•‘æ˜Ÿ", "ç¥å™¨", "å˜´é¦‹", "æ‰‹æ®‹å…š", "é“²å±å®˜"]
}

def load_base_model():
    """åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆä¸å¸¦ LoRAï¼‰"""
    print("ğŸ”§ åŠ è½½åŸºç¡€æ¨¡å‹ï¼ˆä¸å¸¦ LoRAï¼‰...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    model.eval()
    return model, tokenizer

def load_lora_model():
    """åŠ è½½å¸¦ LoRA çš„æ¨¡å‹"""
    print("ğŸ”§ åŠ è½½åŸºç¡€æ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    print("ğŸ”§ åŠ è½½ LoRA é€‚é…å™¨...")
    model = PeftModel.from_pretrained(model, LORA_PATH)
    model.eval()
    return model, tokenizer

def generate_text(model, tokenizer, features, max_new_tokens=180, temperature=0.7, top_p=0.9):
    """ç”Ÿæˆæ–‡æ¡ˆ"""
    prompt = f"è¯·æ ¹æ®å•†å“ç‰¹å¾å†™ä¸€ä¸ªå°çº¢ä¹¦é£æ ¼çš„æ–‡æ¡ˆï¼š\nå•†å“ï¼š{features}\næ–‡æ¡ˆï¼š"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=True
        )
    
    # åªè¿”å›ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰ promptï¼‰
    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # æå–ç”Ÿæˆçš„éƒ¨åˆ†
    if "æ–‡æ¡ˆï¼š" in full_text:
        generated = full_text.split("æ–‡æ¡ˆï¼š")[-1].strip()
    else:
        generated = full_text[len(prompt):].strip()
    
    return generated

def evaluate_xhs_style(text):
    """è¯„ä¼°æ–‡æœ¬çš„å°çº¢ä¹¦é£æ ¼å¾—åˆ†ï¼ˆ0-100åˆ†ï¼‰"""
    score = 0
    features = {
        "has_emoji": False,
        "emoji_count": 0,
        "has_exclamation": False,
        "exclamation_count": 0,
        "has_emotional": False,
        "emotional_count": 0,
        "has_ending_marker": False,
        "has_oral": False,
        "oral_count": 0,
        "length_score": 0
    }
    
    # 1. Emoji æ£€æµ‹ï¼ˆ30åˆ†ï¼‰
    emoji_pattern = r'[\U0001F300-\U0001F9FF]|[\u2600-\u27FF]'
    emojis = re.findall(emoji_pattern, text)
    features["emoji_count"] = len(emojis)
    features["has_emoji"] = len(emojis) > 0
    if features["has_emoji"]:
        score += 30
        if features["emoji_count"] >= 2:
            score += 5  # å¤šä¸ª emoji åŠ åˆ†
    
    # 2. æ„Ÿå¹è¯æ£€æµ‹ï¼ˆ25åˆ†ï¼‰
    for word in XHS_STYLE_KEYWORDS["exclamations"]:
        if word in text:
            features["has_exclamation"] = True
            features["exclamation_count"] += text.count(word)
    if features["has_exclamation"]:
        score += 25
    
    # 3. æƒ…æ„Ÿè¯æ±‡æ£€æµ‹ï¼ˆ20åˆ†ï¼‰
    for word in XHS_STYLE_KEYWORDS["emotional_words"]:
        if word in text:
            features["has_emotional"] = True
            features["emotional_count"] += text.count(word)
    if features["has_emotional"]:
        score += 20
    
    # 4. ç»“å°¾æ ‡è®°ï¼ˆ10åˆ†ï¼‰
    for marker in XHS_STYLE_KEYWORDS["ending_markers"]:
        if text.endswith(marker):
            features["has_ending_marker"] = True
            score += 10
            break
    
    # 5. å£è¯­åŒ–è¡¨è¾¾ï¼ˆ10åˆ†ï¼‰
    for expr in XHS_STYLE_KEYWORDS["oral_expressions"]:
        if expr in text:
            features["has_oral"] = True
            features["oral_count"] += text.count(expr)
    if features["has_oral"]:
        score += 10
    
    # 6. é•¿åº¦è¯„åˆ†ï¼ˆ5åˆ†ï¼‰- å°çº¢ä¹¦æ–‡æ¡ˆé€šå¸¸åœ¨ 25-60 å­—ç¬¦
    length = len(text)
    if 25 <= length <= 60:
        features["length_score"] = 5
        score += 5
    elif 20 <= length < 25 or 60 < length <= 80:
        features["length_score"] = 3
        score += 3
    
    # é™åˆ¶æœ€é«˜åˆ†
    score = min(score, 100)
    
    return score, features

def load_training_samples():
    """åŠ è½½è®­ç»ƒæ ·æœ¬ç”¨äºå‚è€ƒ"""
    samples = []
    try:
        with open("train.json", "r", encoding="utf-8") as f:
            for line in f:
                obj = json.loads(line)
                samples.append(obj["output"])
    except:
        pass
    return samples

def calculate_similarity_to_training(text, training_samples):
    """è®¡ç®—ä¸è®­ç»ƒæ ·æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆåŸºäºå…±åŒè¯æ±‡ï¼‰"""
    if not training_samples:
        return 0
    
    # æå–æ–‡æœ¬ä¸­çš„å…³é”®è¯ï¼ˆå»é™¤æ ‡ç‚¹å’Œå¸¸è§è¯ï¼‰
    def extract_keywords(t):
        # ç§»é™¤ emoji å’Œæ ‡ç‚¹
        t_clean = re.sub(r'[\U0001F300-\U0001F9FF]|[\u2600-\u27FF]', '', t)
        t_clean = re.sub(r'[ï¼Œã€‚ï¼ï¼Ÿï½\s]', '', t_clean)
        return set(t_clean)
    
    text_keywords = extract_keywords(text)
    
    # è®¡ç®—ä¸æ‰€æœ‰è®­ç»ƒæ ·æœ¬çš„å¹³å‡ç›¸ä¼¼åº¦
    similarities = []
    for sample in training_samples:
        sample_keywords = extract_keywords(sample)
        if len(text_keywords) == 0 or len(sample_keywords) == 0:
            continue
        intersection = len(text_keywords & sample_keywords)
        union = len(text_keywords | sample_keywords)
        if union > 0:
            similarities.append(intersection / union)
    
    return sum(similarities) / len(similarities) * 100 if similarities else 0

def compare_single(features, base_model, base_tokenizer, lora_model, lora_tokenizer, training_samples=None):
    """å¯¹æ¯”å•ä¸ªè¾“å…¥çš„ç”Ÿæˆç»“æœ"""
    print(f"\n{'='*80}")
    print(f"ğŸ“¦ å•†å“ç‰¹å¾: {features}")
    print(f"{'='*80}")
    
    # åŸºç¡€æ¨¡å‹ç”Ÿæˆ
    print("\nğŸ”µ ã€åŸºç¡€æ¨¡å‹ï¼ˆæ—  LoRAï¼‰ã€‘")
    base_result = generate_text(base_model, base_tokenizer, features)
    print(base_result)
    
    # LoRA æ¨¡å‹ç”Ÿæˆ
    print("\nğŸŸ¢ ã€LoRA å¾®è°ƒæ¨¡å‹ã€‘")
    lora_result = generate_text(lora_model, lora_tokenizer, features)
    print(lora_result)
    
    # è¯¦ç»†è¯„ä¼°åˆ†æ
    print(f"\nğŸ“Š è¯¦ç»†è¯„ä¼°åˆ†æ:")
    print(f"{'-'*80}")
    
    # é£æ ¼è¯„åˆ†
    base_score, base_features = evaluate_xhs_style(base_result)
    lora_score, lora_features = evaluate_xhs_style(lora_result)
    
    print(f"\nğŸ¯ å°çº¢ä¹¦é£æ ¼è¯„åˆ† (0-100åˆ†):")
    print(f"  åŸºç¡€æ¨¡å‹: {base_score:.1f} åˆ†")
    print(f"  LoRA æ¨¡å‹: {lora_score:.1f} åˆ†")
    print(f"  æå‡: {lora_score - base_score:+.1f} åˆ†")
    
    # è¯¦ç»†ç‰¹å¾å¯¹æ¯”
    print(f"\nğŸ“ é£æ ¼ç‰¹å¾å¯¹æ¯”:")
    print(f"  Emoji: åŸºç¡€={base_features['emoji_count']}ä¸ª, LoRA={lora_features['emoji_count']}ä¸ª")
    print(f"  æ„Ÿå¹è¯: åŸºç¡€={base_features['exclamation_count']}ä¸ª, LoRA={lora_features['exclamation_count']}ä¸ª")
    print(f"  æƒ…æ„Ÿè¯: åŸºç¡€={base_features['emotional_count']}ä¸ª, LoRA={lora_features['emotional_count']}ä¸ª")
    print(f"  å£è¯­åŒ–: åŸºç¡€={base_features['oral_count']}ä¸ª, LoRA={lora_features['oral_count']}ä¸ª")
    print(f"  ç»“å°¾æ ‡è®°: åŸºç¡€={'âœ“' if base_features['has_ending_marker'] else 'âœ—'}, LoRA={'âœ“' if lora_features['has_ending_marker'] else 'âœ—'}")
    print(f"  é•¿åº¦: åŸºç¡€={len(base_result)}å­—ç¬¦, LoRA={len(lora_result)}å­—ç¬¦")
    
    # ä¸è®­ç»ƒæ•°æ®ç›¸ä¼¼åº¦
    if training_samples:
        base_sim = calculate_similarity_to_training(base_result, training_samples)
        lora_sim = calculate_similarity_to_training(lora_result, training_samples)
        print(f"\nğŸ“š ä¸è®­ç»ƒæ•°æ®ç›¸ä¼¼åº¦:")
        print(f"  åŸºç¡€æ¨¡å‹: {base_sim:.1f}%")
        print(f"  LoRA æ¨¡å‹: {lora_sim:.1f}%")
        print(f"  æå‡: {lora_sim - base_sim:+.1f}%")
    
    # ç»¼åˆåˆ¤æ–­
    print(f"\n{'='*80}")
    if lora_score > base_score + 10:
        print("âœ… LoRA å¾®è°ƒæ•ˆæœæ˜¾è‘—ï¼é£æ ¼æ›´æ¥è¿‘å°çº¢ä¹¦æ–‡æ¡ˆ")
    elif lora_score > base_score + 5:
        print("âš ï¸  LoRA å¾®è°ƒæœ‰ä¸€å®šæ•ˆæœï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´")
    elif lora_score > base_score:
        print("âš ï¸  LoRA å¾®è°ƒæ•ˆæœå¾®å¼±ï¼Œå¯èƒ½éœ€è¦æ›´å¤šè®­ç»ƒæ•°æ®æˆ–è°ƒæ•´å‚æ•°")
    else:
        print("âŒ LoRA å¾®è°ƒæ•ˆæœä¸æ˜æ˜¾ï¼Œå»ºè®®æ£€æŸ¥è®­ç»ƒè¿‡ç¨‹")
    print(f"{'='*80}")
    
    return {
        "base": base_result,
        "lora": lora_result,
        "base_score": base_score,
        "lora_score": lora_score,
        "base_features": base_features,
        "lora_features": lora_features
    }

def batch_compare(test_file="test_prompts.json", num_samples=5):
    """æ‰¹é‡å¯¹æ¯”æµ‹è¯•"""
    print("="*80)
    print("ğŸš€ LoRA å¾®è°ƒæ•ˆæœå¯¹æ¯”æµ‹è¯•")
    print("="*80)
    
    # åŠ è½½æ¨¡å‹
    base_model, base_tokenizer = load_base_model()
    lora_model, lora_tokenizer = load_lora_model()
    
    # åŠ è½½è®­ç»ƒæ ·æœ¬ç”¨äºå‚è€ƒ
    training_samples = load_training_samples()
    if training_samples:
        print(f"ğŸ“š å·²åŠ è½½ {len(training_samples)} ä¸ªè®­ç»ƒæ ·æœ¬ç”¨äºå‚è€ƒ")
    
    # è¯»å–æµ‹è¯•ç”¨ä¾‹
    with open(test_file, "r", encoding="utf-8") as f:
        test_cases = json.load(f)
    
    # é™åˆ¶æµ‹è¯•æ•°é‡
    test_cases = test_cases[:num_samples]
    
    print(f"\nğŸ“ å°†æµ‹è¯• {len(test_cases)} ä¸ªç”¨ä¾‹\n")
    
    results = []
    for i, features in enumerate(test_cases, 1):
        print(f"\nã€æµ‹è¯• {i}/{len(test_cases)}ã€‘")
        result = compare_single(
            features, base_model, base_tokenizer, lora_model, lora_tokenizer, training_samples
        )
        results.append({
            "features": features,
            **result
        })
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("ğŸ“ˆ ç»¼åˆè¯„ä¼°æ€»ç»“")
    print("="*80)
    
    # åŸºç¡€ç»Ÿè®¡
    base_avg_len = sum(len(r["base"]) for r in results) / len(results)
    lora_avg_len = sum(len(r["lora"]) for r in results) / len(results)
    
    base_avg_score = sum(r["base_score"] for r in results) / len(results)
    lora_avg_score = sum(r["lora_score"] for r in results) / len(results)
    
    # Emoji ç»Ÿè®¡
    base_emoji_count = sum(1 for r in results if r["base_features"]["has_emoji"])
    lora_emoji_count = sum(1 for r in results if r["lora_features"]["has_emoji"])
    
    # æ„Ÿå¹è¯ç»Ÿè®¡
    base_excl_count = sum(1 for r in results if r["base_features"]["has_exclamation"])
    lora_excl_count = sum(1 for r in results if r["lora_features"]["has_exclamation"])
    
    # æƒ…æ„Ÿè¯ç»Ÿè®¡
    base_emo_count = sum(1 for r in results if r["base_features"]["has_emotional"])
    lora_emo_count = sum(1 for r in results if r["lora_features"]["has_emotional"])
    
    # ç»“å°¾æ ‡è®°ç»Ÿè®¡
    base_end_count = sum(1 for r in results if r["base_features"]["has_ending_marker"])
    lora_end_count = sum(1 for r in results if r["lora_features"]["has_ending_marker"])
    
    print(f"\nğŸ¯ å¹³å‡é£æ ¼è¯„åˆ†:")
    print(f"  åŸºç¡€æ¨¡å‹: {base_avg_score:.1f} åˆ†")
    print(f"  LoRA æ¨¡å‹: {lora_avg_score:.1f} åˆ†")
    print(f"  å¹³å‡æå‡: {lora_avg_score - base_avg_score:+.1f} åˆ†")
    
    print(f"\nğŸ“ å¹³å‡æ–‡æ¡ˆé•¿åº¦:")
    print(f"  åŸºç¡€æ¨¡å‹: {base_avg_len:.1f} å­—ç¬¦")
    print(f"  LoRA æ¨¡å‹: {lora_avg_len:.1f} å­—ç¬¦")
    print(f"  å·®å¼‚: {lora_avg_len - base_avg_len:+.1f} å­—ç¬¦")
    
    print(f"\nâœ¨ é£æ ¼ç‰¹å¾è¦†ç›–ç‡:")
    print(f"  Emoji: åŸºç¡€={base_emoji_count}/{len(results)} ({base_emoji_count/len(results)*100:.1f}%), LoRA={lora_emoji_count}/{len(results)} ({lora_emoji_count/len(results)*100:.1f}%)")
    print(f"  æ„Ÿå¹è¯: åŸºç¡€={base_excl_count}/{len(results)} ({base_excl_count/len(results)*100:.1f}%), LoRA={lora_excl_count}/{len(results)} ({lora_excl_count/len(results)*100:.1f}%)")
    print(f"  æƒ…æ„Ÿè¯: åŸºç¡€={base_emo_count}/{len(results)} ({base_emo_count/len(results)*100:.1f}%), LoRA={lora_emo_count}/{len(results)} ({lora_emo_count/len(results)*100:.1f}%)")
    print(f"  ç»“å°¾æ ‡è®°: åŸºç¡€={base_end_count}/{len(results)} ({base_end_count/len(results)*100:.1f}%), LoRA={lora_end_count}/{len(results)} ({lora_end_count/len(results)*100:.1f}%)")
    
    # æœ€ç»ˆè¯„ä¼°
    print(f"\n{'='*80}")
    print("ğŸ“ å¾®è°ƒæ•ˆæœè¯„ä¼°")
    print(f"{'='*80}")
    
    score_improvement = lora_avg_score - base_avg_score
    emoji_improvement = (lora_emoji_count - base_emoji_count) / len(results) * 100
    
    if score_improvement >= 15 and emoji_improvement >= 20:
        print("âœ… ä¼˜ç§€ï¼LoRA å¾®è°ƒæ•ˆæœéå¸¸æ˜¾è‘—")
        print("   - é£æ ¼è¯„åˆ†å¤§å¹…æå‡")
        print("   - å°çº¢ä¹¦ç‰¹å¾æ˜æ˜¾å¢å¼º")
        print("   - å»ºè®®ï¼šå¯ä»¥ç»§ç»­ä½¿ç”¨å½“å‰æ¨¡å‹")
    elif score_improvement >= 10 and emoji_improvement >= 10:
        print("âœ… è‰¯å¥½ï¼LoRA å¾®è°ƒæœ‰æ˜æ˜¾æ•ˆæœ")
        print("   - é£æ ¼è¯„åˆ†æœ‰æ‰€æå‡")
        print("   - éƒ¨åˆ†ç‰¹å¾å¾—åˆ°æ”¹å–„")
        print("   - å»ºè®®ï¼šå¯ä»¥å°è¯•å¢åŠ è®­ç»ƒæ­¥æ•°æˆ–è°ƒæ•´å­¦ä¹ ç‡")
    elif score_improvement >= 5:
        print("âš ï¸  ä¸€èˆ¬ï¼LoRA å¾®è°ƒæ•ˆæœæœ‰é™")
        print("   - é£æ ¼è¯„åˆ†ç•¥æœ‰æå‡")
        print("   - ç‰¹å¾æ”¹å–„ä¸æ˜æ˜¾")
        print("   - å»ºè®®ï¼šå¢åŠ è®­ç»ƒæ•°æ®ã€è°ƒæ•´ LoRA å‚æ•°ï¼ˆrå€¼ï¼‰æˆ–å¢åŠ è®­ç»ƒæ­¥æ•°")
    else:
        print("âŒ è¾ƒå·®ï¼LoRA å¾®è°ƒæ•ˆæœä¸æ˜æ˜¾")
        print("   - é£æ ¼è¯„åˆ†æå‡å¾ˆå°æˆ–æ²¡æœ‰æå‡")
        print("   - å»ºè®®ï¼š")
        print("     1. æ£€æŸ¥è®­ç»ƒæ•°æ®è´¨é‡å’Œæ•°é‡ï¼ˆå»ºè®®è‡³å°‘ 100+ æ ·æœ¬ï¼‰")
        print("     2. å¢åŠ è®­ç»ƒæ­¥æ•°ï¼ˆå½“å‰ 200 æ­¥å¯èƒ½ä¸å¤Ÿï¼‰")
        print("     3. è°ƒæ•´ LoRA å‚æ•°ï¼šr=16, alpha=32")
        print("     4. æ£€æŸ¥è®­ç»ƒæŸå¤±æ˜¯å¦æ­£å¸¸ä¸‹é™")
    
    print(f"{'='*80}")
    
    return results

def interactive_compare():
    """äº¤äº’å¼å¯¹æ¯”æµ‹è¯•"""
    print("="*80)
    print("ğŸš€ LoRA å¾®è°ƒæ•ˆæœå¯¹æ¯”æµ‹è¯•ï¼ˆäº¤äº’æ¨¡å¼ï¼‰")
    print("="*80)
    
    # åŠ è½½æ¨¡å‹
    base_model, base_tokenizer = load_base_model()
    lora_model, lora_tokenizer = load_lora_model()
    
    print("\nâœ… æ¨¡å‹åŠ è½½å®Œæˆï¼å¯ä»¥å¼€å§‹å¯¹æ¯”æµ‹è¯•äº†ã€‚")
    print("ğŸ’¡ æç¤ºï¼šç›´æ¥å›è½¦ä½¿ç”¨é»˜è®¤æµ‹è¯•ç”¨ä¾‹ï¼Œæˆ–è¾“å…¥è‡ªå®šä¹‰å•†å“ç‰¹å¾")
    
    # åŠ è½½è®­ç»ƒæ ·æœ¬ç”¨äºå‚è€ƒ
    training_samples = load_training_samples()
    
    while True:
        features = input("\nğŸ“¦ è¾“å…¥å•†å“ç‰¹å¾ï¼ˆå›è½¦é€€å‡ºï¼‰: ").strip()
        if not features:
            break
        
        compare_single(features, base_model, base_tokenizer, lora_model, lora_tokenizer, training_samples)

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--interactive":
        # äº¤äº’æ¨¡å¼
        interactive_compare()
    else:
        # æ‰¹é‡æµ‹è¯•æ¨¡å¼
        num_samples = int(sys.argv[1]) if len(sys.argv) > 1 else 5
        batch_compare(num_samples=num_samples)

