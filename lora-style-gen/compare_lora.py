import json
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

BASE_MODEL = "Qwen/Qwen2-0.5B-Instruct"
LORA_PATH = "./output_lora"

def load_base_model():
    """åŠ è½½åŸºç¡€æ¨¡åž‹ï¼ˆä¸å¸¦ LoRAï¼‰"""
    print("ðŸ”§ åŠ è½½åŸºç¡€æ¨¡åž‹ï¼ˆä¸å¸¦ LoRAï¼‰...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    model.eval()
    return model, tokenizer

def load_lora_model():
    """åŠ è½½å¸¦ LoRA çš„æ¨¡åž‹"""
    print("ðŸ”§ åŠ è½½åŸºç¡€æ¨¡åž‹...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float16,
        device_map="auto"
    )
    print("ðŸ”§ åŠ è½½ LoRA é€‚é…å™¨...")
    model = PeftModel.from_pretrained(model, LORA_PATH)
    model.eval()
    return model, tokenizer

def generate_text(model, tokenizer, features, max_new_tokens=180, temperature=0.7, top_p=0.9):
    """ç”Ÿæˆæ–‡æ¡ˆ"""
    prompt = f"è¯·æ ¹æ®å•†å“ç‰¹å¾å†™ä¸€ä¸ªå°çº¢ä¹¦é£Žæ ¼çš„æ–‡æ¡ˆï¼š\nå•†å“ï¼š{features}\næ–‡æ¡ˆï¼š"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=True
        )
    
    # åªè¿”å›žç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆåŽ»æŽ‰ promptï¼‰
    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # æå–ç”Ÿæˆçš„éƒ¨åˆ†
    if "æ–‡æ¡ˆï¼š" in full_text:
        generated = full_text.split("æ–‡æ¡ˆï¼š")[-1].strip()
    else:
        generated = full_text[len(prompt):].strip()
    
    return generated

def compare_single(features, base_model, base_tokenizer, lora_model, lora_tokenizer):
    """å¯¹æ¯”å•ä¸ªè¾“å…¥çš„ç”Ÿæˆç»“æžœ"""
    print(f"\n{'='*80}")
    print(f"ðŸ“¦ å•†å“ç‰¹å¾: {features}")
    print(f"{'='*80}")
    
    # åŸºç¡€æ¨¡åž‹ç”Ÿæˆ
    print("\nðŸ”µ ã€åŸºç¡€æ¨¡åž‹ï¼ˆæ—  LoRAï¼‰ã€‘")
    base_result = generate_text(base_model, base_tokenizer, features)
    print(base_result)
    
    # LoRA æ¨¡åž‹ç”Ÿæˆ
    print("\nðŸŸ¢ ã€LoRA å¾®è°ƒæ¨¡åž‹ã€‘")
    lora_result = generate_text(lora_model, lora_tokenizer, features)
    print(lora_result)
    
    # ç®€å•å¯¹æ¯”åˆ†æž
    print(f"\nðŸ“Š å¯¹æ¯”åˆ†æž:")
    print(f"  åŸºç¡€æ¨¡åž‹é•¿åº¦: {len(base_result)} å­—ç¬¦")
    print(f"  LoRA æ¨¡åž‹é•¿åº¦: {len(lora_result)} å­—ç¬¦")
    print(f"  æ˜¯å¦åŒ…å« emoji: åŸºç¡€={('ðŸ”¥' in base_result or 'âœ¨' in base_result or 'ðŸ’—' in base_result)}, LoRA={('ðŸ”¥' in lora_result or 'âœ¨' in lora_result or 'ðŸ’—' in lora_result)}")
    
    return base_result, lora_result

def batch_compare(test_file="test_prompts.json", num_samples=5):
    """æ‰¹é‡å¯¹æ¯”æµ‹è¯•"""
    print("="*80)
    print("ðŸš€ LoRA å¾®è°ƒæ•ˆæžœå¯¹æ¯”æµ‹è¯•")
    print("="*80)
    
    # åŠ è½½æ¨¡åž‹
    base_model, base_tokenizer = load_base_model()
    lora_model, lora_tokenizer = load_lora_model()
    
    # è¯»å–æµ‹è¯•ç”¨ä¾‹
    with open(test_file, "r", encoding="utf-8") as f:
        test_cases = json.load(f)
    
    # é™åˆ¶æµ‹è¯•æ•°é‡
    test_cases = test_cases[:num_samples]
    
    print(f"\nðŸ“ å°†æµ‹è¯• {len(test_cases)} ä¸ªç”¨ä¾‹\n")
    
    results = []
    for i, features in enumerate(test_cases, 1):
        print(f"\nã€æµ‹è¯• {i}/{len(test_cases)}ã€‘")
        base_result, lora_result = compare_single(
            features, base_model, base_tokenizer, lora_model, lora_tokenizer
        )
        results.append({
            "features": features,
            "base": base_result,
            "lora": lora_result
        })
    
    # æ€»ç»“
    print("\n" + "="*80)
    print("ðŸ“ˆ æµ‹è¯•æ€»ç»“")
    print("="*80)
    
    base_avg_len = sum(len(r["base"]) for r in results) / len(results)
    lora_avg_len = sum(len(r["lora"]) for r in results) / len(results)
    
    base_emoji_count = sum(1 for r in results if any(emoji in r["base"] for emoji in ["ðŸ”¥", "âœ¨", "ðŸ’—", "ðŸ’Ž", "ðŸ›", "â„ï¸"]))
    lora_emoji_count = sum(1 for r in results if any(emoji in r["lora"] for emoji in ["ðŸ”¥", "âœ¨", "ðŸ’—", "ðŸ’Ž", "ðŸ›", "â„ï¸"]))
    
    print(f"\nå¹³å‡æ–‡æ¡ˆé•¿åº¦:")
    print(f"  åŸºç¡€æ¨¡åž‹: {base_avg_len:.1f} å­—ç¬¦")
    print(f"  LoRA æ¨¡åž‹: {lora_avg_len:.1f} å­—ç¬¦")
    print(f"  å·®å¼‚: {lora_avg_len - base_avg_len:+.1f} å­—ç¬¦")
    
    print(f"\nåŒ…å« emoji çš„æ ·æœ¬æ•°:")
    print(f"  åŸºç¡€æ¨¡åž‹: {base_emoji_count}/{len(results)} ({base_emoji_count/len(results)*100:.1f}%)")
    print(f"  LoRA æ¨¡åž‹: {lora_emoji_count}/{len(results)} ({lora_emoji_count/len(results)*100:.1f}%)")
    
    return results

def interactive_compare():
    """äº¤äº’å¼å¯¹æ¯”æµ‹è¯•"""
    print("="*80)
    print("ðŸš€ LoRA å¾®è°ƒæ•ˆæžœå¯¹æ¯”æµ‹è¯•ï¼ˆäº¤äº’æ¨¡å¼ï¼‰")
    print("="*80)
    
    # åŠ è½½æ¨¡åž‹
    base_model, base_tokenizer = load_base_model()
    lora_model, lora_tokenizer = load_lora_model()
    
    print("\nâœ… æ¨¡åž‹åŠ è½½å®Œæˆï¼å¯ä»¥å¼€å§‹å¯¹æ¯”æµ‹è¯•äº†ã€‚")
    print("ðŸ’¡ æç¤ºï¼šç›´æŽ¥å›žè½¦ä½¿ç”¨é»˜è®¤æµ‹è¯•ç”¨ä¾‹ï¼Œæˆ–è¾“å…¥è‡ªå®šä¹‰å•†å“ç‰¹å¾")
    
    while True:
        features = input("\nðŸ“¦ è¾“å…¥å•†å“ç‰¹å¾ï¼ˆå›žè½¦é€€å‡ºï¼‰: ").strip()
        if not features:
            break
        
        compare_single(features, base_model, base_tokenizer, lora_model, lora_tokenizer)

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) > 1 and sys.argv[1] == "--interactive":
        # äº¤äº’æ¨¡å¼
        interactive_compare()
    else:
        # æ‰¹é‡æµ‹è¯•æ¨¡å¼
        num_samples = int(sys.argv[1]) if len(sys.argv) > 1 else 5
        batch_compare(num_samples=num_samples)

