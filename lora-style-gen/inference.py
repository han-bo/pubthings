import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

BASE_MODEL = "Qwen/Qwen2-0.5B-Instruct"
LORA_PATH = "./output_lora"   # ä½ çš„ LoRA æƒé‡è·¯å¾„

print("ğŸ”§ Loading tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, trust_remote_code=True)

print("ğŸ”§ Loading base model...")
model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.float16,
    device_map="auto"
)

print("ğŸ”§ Loading LoRA adapter...")
model = PeftModel.from_pretrained(model, LORA_PATH)
model.eval()


def generate_xhs_text(features, max_new_tokens=180, temperature=0.7, top_p=0.9):
    """
    features: å•†å“ç‰¹å¾ï¼Œä¾‹å¦‚ "è“ç‰™è€³æœºï¼Œç»­èˆªé•¿"
    """
    prompt = f"è¯·æ ¹æ®å•†å“ç‰¹å¾å†™ä¸€ä¸ªå°çº¢ä¹¦é£æ ¼çš„æ–‡æ¡ˆï¼š\nå•†å“ï¼š{features}\næ–‡æ¡ˆï¼š"

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            top_p=top_p,
            do_sample=True
        )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


if __name__ == "__main__":
    print("=== å°çº¢ä¹¦ LoRA æ¨¡å‹æµ‹è¯• ===")
    while True:
        features = input("\nè¾“å…¥å•†å“ç‰¹å¾ï¼š")
        if not features.strip():
            continue
        print("\nğŸ“Œ ç”Ÿæˆç»“æœï¼š")
        print(generate_xhs_text(features))

