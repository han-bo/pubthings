import json
import os
from dataclasses import dataclass

import torch
from torch.utils.data import Dataset

from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
)

from peft import LoraConfig, get_peft_model


# =====================
# 1. å‡†å¤‡æ•°æ®é›†
# =====================

class JsonlDataset(Dataset):
    def __init__(self, data_path, tokenizer, max_len=512):
        self.tokenizer = tokenizer
        self.max_len = max_len

        self.samples = []
        with open(data_path, "r", encoding="utf-8") as f:
            for line in f:
                obj = json.loads(line)
                instruction = obj["instruction"]
                _input = obj["input"]
                output = obj["output"]

                # æ„å»º prompt æ ¼å¼ï¼šä½ å¯ä»¥æ ¹æ®æ¨¡å‹æ ¼å¼è°ƒæ•´
                prompt = f"{instruction}\nè¾“å…¥ï¼š{_input}\nè¾“å‡ºï¼š"
                target = output

                self.samples.append((prompt, target))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        prompt, target = self.samples[idx]
        
        # æ¨¡å‹è®­ç»ƒæ ¼å¼ä¸º prompt + answer
        full_text = prompt + target
        
        tokenized = self.tokenizer(
            full_text,
            truncation=True,
            max_length=self.max_len,
            padding=False,
        )
        return {
            "input_ids": torch.tensor(tokenized["input_ids"]),
            "attention_mask": torch.tensor(tokenized["attention_mask"]),
            "labels": torch.tensor(tokenized["input_ids"]),  # causal LM ç›´æ¥é¢„æµ‹ä¸‹ä¸€ä¸ª token
        }


# =====================
# 2. åŠ è½½æ¨¡å‹ä¸ LoRA
# =====================

def load_model_and_tokenizer(base_model):
    tokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    
    model = AutoModelForCausalLM.from_pretrained(
        base_model,
        torch_dtype=torch.float16,
        device_map="auto",
    )

    # LoRA é…ç½®
    lora_config = LoraConfig(
        r=6,
        lora_alpha=8,
        target_modules=["q_proj", "v_proj"],   # å¯¹ Qwen éå¸¸é€‚ç”¨
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM",
    )

    model = get_peft_model(model, lora_config)
    model.print_trainable_parameters()
    return model, tokenizer


# =====================
# 3. å¼€å§‹è®­ç»ƒ
# =====================

def train():
    base_model = "Qwen/Qwen2-0.5B-Instruct"
    #train_file = "train.json"
    train_file = "xiaohongshu_200.jsonl"
    output_dir = "output_lora"

    model, tokenizer = load_model_and_tokenizer(base_model)

    dataset = JsonlDataset(train_file, tokenizer)
    data_collator = DataCollatorForSeq2Seq(tokenizer, padding=True)

    training_args = TrainingArguments(
        output_dir=output_dir,
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        warmup_steps=10,
        max_steps=200,   # å°‘é‡æ­¥éª¤å³å¯è·‘é€š
        learning_rate=4e-5,
        fp16=True,
        logging_steps=10,
        save_steps=100,
        save_total_limit=2,
        optim="adamw_torch",
        max_grad_norm=1.0,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=data_collator,
    )

    trainer.train()

    # ä¿å­˜ LoRA adapter
    model.save_pretrained(output_dir)
    print("ğŸ‰ LoRA è®­ç»ƒå®Œæˆï¼æƒé‡å·²ä¿å­˜åˆ° output_lora/")


if __name__ == "__main__":
    train()

