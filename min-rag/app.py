import os
import sys
import requests
import warnings
import logging
from pathlib import Path

# ç¦ç”¨ ChromaDB é¥æµ‹åŠŸèƒ½ï¼ˆé¿å…é”™è¯¯ä¿¡æ¯ï¼‰
# å¿…é¡»åœ¨å¯¼å…¥ Chroma ä¹‹å‰è®¾ç½®
os.environ["ANONYMIZED_TELEMETRY"] = "False"
# å¿½ç•¥ ChromaDB é¥æµ‹ç›¸å…³çš„è­¦å‘Š
warnings.filterwarnings(
    "ignore",
    message=".*telemetry.*",
    category=UserWarning
)

# ä»¥ä¸‹å¯¼å…¥å¿…é¡»åœ¨ç¯å¢ƒå˜é‡è®¾ç½®ä¹‹å
# ä»¥ä¸‹å¯¼å…¥å¿…é¡»åœ¨ç¯å¢ƒå˜é‡è®¾ç½®ä¹‹å
from langchain_community.document_loaders import (  # noqa: E402
    Docx2txtLoader
)
from langchain_text_splitters import (  # noqa: E402
    RecursiveCharacterTextSplitter
)
from langchain_community.embeddings import (  # noqa: E402
    SentenceTransformerEmbeddings
)
from langchain_community.vectorstores import Chroma  # noqa: E402
from langchain_community.llms import Ollama  # noqa: E402
from langchain.prompts import PromptTemplate  # noqa: E402
from langchain.chains import RetrievalQA  # noqa: E402


def setup_logging(log_dir="logs"):
    """
    é…ç½®æ—¥å¿—ç³»ç»Ÿ
    
    æ—¥å¿—ä¼šè®°å½•åˆ°ï¼š
    - logs/rag_conversations.log: å¯¹è¯æ—¥å¿—ï¼ˆé—®é¢˜ã€ç­”æ¡ˆã€æ£€ç´¢å†…å®¹ç­‰ï¼‰
    - logs/rag_debug.log: è°ƒè¯•æ—¥å¿—ï¼ˆè¯¦ç»†çš„ promptã€å‘é‡ä¿¡æ¯ç­‰ï¼‰
    
    Returns:
        logger: é…ç½®å¥½çš„æ—¥å¿—è®°å½•å™¨
    """
    # åˆ›å»ºæ—¥å¿—ç›®å½•
    Path(log_dir).mkdir(exist_ok=True)
    
    # é…ç½®ä¸»æ—¥å¿—ï¼ˆå¯¹è¯è®°å½•ï¼‰
    conversation_logger = logging.getLogger("conversation")
    conversation_logger.setLevel(logging.INFO)
    
    # å¦‚æœå·²ç»é…ç½®è¿‡ï¼Œé¿å…é‡å¤æ·»åŠ  handler
    if not conversation_logger.handlers:
        conversation_handler = logging.FileHandler(
            os.path.join(log_dir, "rag_conversations.log"),
            encoding="utf-8"
        )
        conversation_handler.setFormatter(
            logging.Formatter(
                '%(asctime)s - %(levelname)s - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
        )
        conversation_logger.addHandler(conversation_handler)
    
    # é…ç½®è°ƒè¯•æ—¥å¿—ï¼ˆè¯¦ç»†æŠ€æœ¯ä¿¡æ¯ï¼‰
    debug_logger = logging.getLogger("debug")
    debug_logger.setLevel(logging.DEBUG)
    
    if not debug_logger.handlers:
        debug_handler = logging.FileHandler(
            os.path.join(log_dir, "rag_debug.log"),
            encoding="utf-8"
        )
        debug_handler.setFormatter(
            logging.Formatter(
                '%(asctime)s - [DEBUG] - %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
        )
        debug_logger.addHandler(debug_handler)
    
    return conversation_logger, debug_logger


"""
================================================================================
RAG (Retrieval-Augmented Generation) æ£€ç´¢å¢å¼ºç”Ÿæˆç³»ç»Ÿå·¥ä½œæµç¨‹
================================================================================

RAG ç³»ç»Ÿé€šè¿‡ä»¥ä¸‹æ­¥éª¤å®ç°åŸºäºæ–‡æ¡£çš„æ™ºèƒ½é—®ç­”ï¼š

ã€é˜¶æ®µ1ï¼šæ–‡æ¡£é¢„å¤„ç†ï¼ˆç¦»çº¿ï¼Œé¦–æ¬¡è¿è¡Œï¼‰ã€‘
1. æ–‡æ¡£åŠ è½½ (Document Loading)
   - ä»æ–‡ä»¶ç³»ç»ŸåŠ è½½æ–‡æ¡£ï¼ˆWordã€PDFã€TXTç­‰ï¼‰
   - å°†æ–‡æ¡£è½¬æ¢ä¸ºç»Ÿä¸€çš„æ–‡æœ¬æ ¼å¼

2. æ–‡æœ¬åˆ†å— (Text Chunking)
   - å°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆå¤šä¸ªå°ç‰‡æ®µï¼ˆchunksï¼‰
   - æ¯ä¸ªç‰‡æ®µåŒ…å«éƒ¨åˆ†é‡å ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
   - ç›®çš„ï¼šä¾¿äºæ£€ç´¢å’ŒåŒ¹é…

3. å‘é‡åŒ– (Embedding)
   - ä½¿ç”¨åµŒå…¥æ¨¡å‹ï¼ˆå¦‚ SentenceTransformerï¼‰å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
   - æ¯ä¸ªæ–‡æœ¬å—å˜æˆä¸€ä¸ªé«˜ç»´å‘é‡ï¼ˆå¦‚ 384 ç»´ï¼‰
   - è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬ä¼šäº§ç”Ÿç›¸ä¼¼çš„å‘é‡

4. å‘é‡å­˜å‚¨ (Vector Storage)
   - å°†æ‰€æœ‰æ–‡æ¡£å—çš„å‘é‡å­˜å‚¨åˆ°å‘é‡æ•°æ®åº“ï¼ˆChromaDBï¼‰
   - å»ºç«‹ç´¢å¼•ï¼Œæ”¯æŒå¿«é€Ÿç›¸ä¼¼åº¦æœç´¢
   - æŒä¹…åŒ–å­˜å‚¨ï¼Œé¿å…é‡å¤å¤„ç†

ã€é˜¶æ®µ2ï¼šé—®ç­”æµç¨‹ï¼ˆåœ¨çº¿ï¼Œæ¯æ¬¡æŸ¥è¯¢ï¼‰ã€‘
5. é—®é¢˜å‘é‡åŒ–
   - å°†ç”¨æˆ·é—®é¢˜è½¬æ¢ä¸ºå‘é‡ï¼ˆä½¿ç”¨ç›¸åŒçš„åµŒå…¥æ¨¡å‹ï¼‰

6. ç›¸ä¼¼åº¦æ£€ç´¢ (Retrieval)
   - åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢ä¸é—®é¢˜æœ€ç›¸ä¼¼çš„æ–‡æ¡£ç‰‡æ®µ
   - è¿”å› Top-K ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£å—ï¼ˆå¦‚ Top-4ï¼‰

7. ä¸Šä¸‹æ–‡å¢å¼º (Augmentation)
   - å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µä½œä¸ºä¸Šä¸‹æ–‡
   - ä¸ç”¨æˆ·é—®é¢˜ä¸€èµ·æ„å»ºæç¤ºè¯ï¼ˆPromptï¼‰

8. ç”Ÿæˆç­”æ¡ˆ (Generation)
   - å°†å¢å¼ºåçš„æç¤ºè¯è¾“å…¥å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰
   - LLM åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
   - è¿”å›ç­”æ¡ˆå’Œå¼•ç”¨æ¥æº

================================================================================
ä¼˜åŠ¿ï¼š
- å¯ä»¥å›ç­”æ–‡æ¡£ä¸­çš„å…·ä½“å†…å®¹ï¼ˆLLM æœ¬èº«ä¸çŸ¥é“æ–‡æ¡£å†…å®¹ï¼‰
- ç­”æ¡ˆæœ‰æ®å¯æŸ¥ï¼ˆå¯ä»¥è¿½æº¯åˆ°æºæ–‡æ¡£ï¼‰
- æ”¯æŒé•¿æ–‡æ¡£ï¼ˆé€šè¿‡åˆ†å—å¤„ç†ï¼‰
- å¯ä»¥æ›´æ–°çŸ¥è¯†ï¼ˆåªéœ€æ›´æ–°å‘é‡åº“ï¼Œæ— éœ€é‡æ–°è®­ç»ƒæ¨¡å‹ï¼‰
================================================================================
"""


def load_word_document(file_path):
    """
    åŠ è½½ Word æ–‡æ¡£
    - .docx æ ¼å¼ï¼šä½¿ç”¨ Docx2txtLoaderï¼ˆä¸éœ€è¦ LibreOfficeï¼‰
    - .doc æ ¼å¼ï¼šéœ€è¦ LibreOfficeï¼Œå¦‚æœæœªå®‰è£…ä¼šç»™å‡ºæç¤º
    """
    file_ext = os.path.splitext(file_path)[1].lower()

    if file_ext == '.docx':
        # ä½¿ç”¨ Docx2txtLoader åŠ è½½ .docx æ–‡ä»¶ï¼ˆä¸éœ€è¦ LibreOfficeï¼‰
        loader = Docx2txtLoader(file_path)
        return loader.load()
    elif file_ext == '.doc':
        # .doc æ ¼å¼éœ€è¦ LibreOffice
        try:
            from langchain_community.document_loaders import (
                UnstructuredWordDocumentLoader
            )
            loader = UnstructuredWordDocumentLoader(file_path)
            return loader.load()
        except FileNotFoundError as e:
            if 'soffice' in str(e):
                raise ValueError(
                    "âŒ æ£€æµ‹åˆ° .doc æ ¼å¼æ–‡ä»¶ï¼Œéœ€è¦å®‰è£… LibreOfficeã€‚\n\n"
                    "ğŸ“‹ è§£å†³æ–¹æ¡ˆï¼ˆé€‰æ‹©å…¶ä¸€ï¼‰ï¼š\n"
                    "1. ã€æ¨èã€‘å°†æ–‡ä»¶è½¬æ¢ä¸º .docx æ ¼å¼\n"
                    "   - ç”¨ Word æˆ– LibreOffice æ‰“å¼€æ–‡ä»¶\n"
                    "   - å¦å­˜ä¸º .docx æ ¼å¼\n"
                    "   - æ›´æ–°ä»£ç ä¸­çš„æ–‡ä»¶è·¯å¾„\n\n"
                    "2. å®‰è£… LibreOfficeï¼ˆmacOSï¼‰ï¼š\n"
                    "   brew install --cask libreoffice\n\n"
                    f"å½“å‰æ–‡ä»¶ï¼š{file_path}"
                ) from e
            raise
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼š{file_ext}")


def build_vectorstore():
    """
    ã€RAG é˜¶æ®µ1ï¼šæ„å»ºå‘é‡åº“ã€‘
    
    è¿™æ˜¯ RAG ç³»ç»Ÿçš„ç¦»çº¿é¢„å¤„ç†é˜¶æ®µï¼Œåªéœ€è¦åœ¨é¦–æ¬¡è¿è¡Œæˆ–æ–‡æ¡£æ›´æ–°æ—¶æ‰§è¡Œã€‚
    æ„å»ºå¥½çš„å‘é‡åº“ä¼šæŒä¹…åŒ–ä¿å­˜ï¼Œåç»­å¯ä»¥ç›´æ¥ä½¿ç”¨ã€‚
    
    æµç¨‹ï¼š
    1. æ–‡æ¡£åŠ è½½ â†’ 2. æ–‡æœ¬åˆ†å— â†’ 3. å‘é‡åŒ– â†’ 4. å‘é‡å­˜å‚¨
    """
    # ========== æ­¥éª¤1ï¼šæ–‡æ¡£åŠ è½½ ==========
    # ä»æ–‡ä»¶ç³»ç»ŸåŠ è½½æ–‡æ¡£ï¼Œè½¬æ¢ä¸º Document å¯¹è±¡åˆ—è¡¨
    # Document å¯¹è±¡åŒ…å« page_contentï¼ˆæ–‡æœ¬å†…å®¹ï¼‰å’Œ metadataï¼ˆå…ƒæ•°æ®ï¼‰
    file_path = "data/ç®€å†_äº’è”ç½‘ç›¸å…³ä¸šåŠ¡ç‰ˆ-08.docx"
    docs = load_word_document(file_path)
    print(f"âœ… å·²åŠ è½½æ–‡æ¡£ï¼Œå…± {len(docs)} é¡µ")

    # ========== æ­¥éª¤2ï¼šæ–‡æœ¬åˆ†å— ==========
    # å°†é•¿æ–‡æ¡£åˆ‡åˆ†æˆå¤šä¸ªå°ç‰‡æ®µï¼Œä¾¿äºåç»­æ£€ç´¢
    # chunk_size=500: æ¯ä¸ªç‰‡æ®µçº¦ 500 ä¸ªå­—ç¬¦
    # chunk_overlap=50: ç›¸é‚»ç‰‡æ®µé‡å  50 ä¸ªå­—ç¬¦ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
    # ä¾‹å¦‚ï¼šæ–‡æ¡£ "ABCDEFGHIJKLMN" å¯èƒ½è¢«åˆ‡åˆ†ä¸ºï¼š
    #   Chunk1: "ABCDEFGHIJ" (0-500)
    #   Chunk2: "FGHIJKLMNO" (450-950, ä¸ Chunk1 é‡å  50)
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500, chunk_overlap=50
    )
    chunks = splitter.split_documents(docs)
    print(f"âœ… æ–‡æ¡£å·²åˆ†å—ï¼Œå…± {len(chunks)} ä¸ªç‰‡æ®µ")

    # ========== æ­¥éª¤3ï¼šå‘é‡åŒ– ==========
    # ä½¿ç”¨åµŒå…¥æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡
    # all-MiniLM-L6-v2: è½»é‡çº§æ¨¡å‹ï¼Œè¾“å‡º 384 ç»´å‘é‡
    # è¯­ä¹‰ç›¸ä¼¼çš„æ–‡æœ¬ä¼šäº§ç”Ÿç›¸ä¼¼çš„å‘é‡ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦é«˜ï¼‰
    # ä¾‹å¦‚ï¼š"æˆ‘å–œæ¬¢è‹¹æœ" å’Œ "æˆ‘çˆ±åƒè‹¹æœ" çš„å‘é‡ä¼šå¾ˆæ¥è¿‘
    embeddings = SentenceTransformerEmbeddings(
        model_name="all-MiniLM-L6-v2"
    )
    print("âœ… åµŒå…¥æ¨¡å‹å·²åŠ è½½")

    # ========== æ­¥éª¤4ï¼šå‘é‡å­˜å‚¨ ==========
    # å°†æ‰€æœ‰æ–‡æ¡£å—çš„å‘é‡å­˜å‚¨åˆ° ChromaDB å‘é‡æ•°æ®åº“
    # persist_directory: æŒ‡å®šæŒä¹…åŒ–ç›®å½•ï¼Œå‘é‡åº“ä¼šä¿å­˜åˆ°ç£ç›˜
    # è¿™æ ·ä¸‹æ¬¡è¿è¡Œå°±ä¸éœ€è¦é‡æ–°å¤„ç†æ–‡æ¡£äº†
    vectordb = Chroma.from_documents(
        chunks, embeddings, persist_directory="vectordb"
    )
    # æŒä¹…åŒ–ä¿å­˜å‘é‡åº“
    vectordb.persist()
    print("âœ… å‘é‡åº“å·²æ„å»ºå¹¶ä¿å­˜")
    return vectordb


def check_system_resources():
    """æ£€æŸ¥ç³»ç»Ÿèµ„æºå¹¶ç»™å‡ºæ¨¡å‹é€‰æ‹©å»ºè®®"""
    try:
        import psutil
        # è·å–å†…å­˜ä¿¡æ¯ï¼ˆGBï¼‰
        memory = psutil.virtual_memory()
        total_memory_gb = memory.total / (1024**3)
        available_memory_gb = memory.available / (1024**3)

        # å°è¯•è·å– GPU ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        gpu_info = None
        try:
            import subprocess
            result = subprocess.run(
                [
                    "nvidia-smi",
                    "--query-gpu=memory.total",
                    "--format=csv,noheader"
                ],
                capture_output=True,
                text=True,
                timeout=2
            )
            if result.returncode == 0:
                gpu_memory_mb = int(result.stdout.strip().split()[0])
                gpu_info = f"{gpu_memory_mb / 1024:.1f} GB"
        except (FileNotFoundError, subprocess.TimeoutExpired, Exception):
            pass

        print("\nğŸ’» ç³»ç»Ÿèµ„æºä¿¡æ¯ï¼š")
        print(f"   æ€»å†…å­˜ï¼š{total_memory_gb:.1f} GB")
        print(f"   å¯ç”¨å†…å­˜ï¼š{available_memory_gb:.1f} GB")
        if gpu_info:
            print(f"   GPU æ˜¾å­˜ï¼š{gpu_info}")
        else:
            print("   GPUï¼šæœªæ£€æµ‹åˆ° NVIDIA GPUï¼ˆå°†ä½¿ç”¨ CPUï¼‰")

        print("\nğŸ“Š æ¨¡å‹é€‰æ‹©å»ºè®®ï¼š")
        if total_memory_gb >= 32 and gpu_info:
            print("   âœ… æ¨èä½¿ç”¨ï¼šqwen2.5:7bï¼ˆæ€§èƒ½æ›´å¥½ï¼‰")
        elif total_memory_gb >= 16:
            print("   âœ… æ¨èä½¿ç”¨ï¼šqwen2.5:1.5bï¼ˆå½“å‰é…ç½®ï¼‰")
            print("   âš ï¸  å¦‚æœæ˜¾å­˜å……è¶³ï¼Œå¯ä»¥å°è¯•ï¼šqwen2.5:7b")
        else:
            print("   âœ… æ¨èä½¿ç”¨ï¼šqwen2.5:1.5bï¼ˆè½»é‡çº§ï¼Œé€‚åˆä½é…ç½®ï¼‰")
        print()

    except ImportError:
        print("\nğŸ’¡ æç¤ºï¼šå®‰è£… psutil å¯ä»¥æŸ¥çœ‹è¯¦ç»†çš„ç³»ç»Ÿèµ„æºä¿¡æ¯")
        print("   è¿è¡Œï¼špip install psutil\n")
    except Exception as e:
        print(f"\nâš ï¸  æ— æ³•æ£€æŸ¥ç³»ç»Ÿèµ„æºï¼š{str(e)}\n")


def check_ollama_connection(base_url="http://localhost:11434"):
    """æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ"""
    try:
        response = requests.get(f"{base_url}/api/tags", timeout=2)
        if response.status_code == 200:
            return True, None
        return False, f"Ollama æœåŠ¡è¿”å›é”™è¯¯çŠ¶æ€ç : {response.status_code}"
    except requests.exceptions.ConnectionError:
        return False, "æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡"
    except Exception as e:
        return False, f"æ£€æŸ¥ Ollama è¿æ¥æ—¶å‡ºé”™: {str(e)}"


def load_qa_chain(model_name="qwen2.5:1.5b"):
    """
    ã€RAG é˜¶æ®µ2ï¼šåŠ è½½é—®ç­”é“¾ã€‘
    
    æ„å»º RAG ç³»ç»Ÿçš„æ ¸å¿ƒç»„ä»¶ï¼Œå°†æ£€ç´¢å™¨å’Œç”Ÿæˆå™¨ç»„åˆåœ¨ä¸€èµ·ã€‚
    æ¯æ¬¡ç”¨æˆ·æé—®æ—¶ï¼Œè¿™ä¸ªé“¾ä¼šè‡ªåŠ¨æ‰§è¡Œï¼šæ£€ç´¢ â†’ å¢å¼º â†’ ç”Ÿæˆ
    
    Args:
        model_name: Ollama æ¨¡å‹åç§°ï¼Œé»˜è®¤ä¸º qwen2.5:1.5b
                   å¯é€‰å€¼ï¼š
                   - qwen2.5:1.5b (è½»é‡çº§ï¼Œæ¨è)
                   - qwen2.5:7b (éœ€è¦ 8-12GB æ˜¾å­˜)
                   - deepseek-r1:1.5b
                   - llama3.2 ç­‰
    """
    # ========== åŠ è½½å‘é‡æ•°æ®åº“ ==========
    # ä»ç£ç›˜åŠ è½½ä¹‹å‰æ„å»ºå¥½çš„å‘é‡åº“
    # ä½¿ç”¨ç›¸åŒçš„åµŒå…¥æ¨¡å‹ï¼Œç¡®ä¿å‘é‡ç©ºé—´ä¸€è‡´
    embedding_function = SentenceTransformerEmbeddings(
        model_name="all-MiniLM-L6-v2"
    )
    vectordb = Chroma(
        persist_directory="vectordb",
        embedding_function=embedding_function
    )
    
    # ========== åˆ›å»ºæ£€ç´¢å™¨ ==========
    # æ£€ç´¢å™¨è´Ÿè´£æ ¹æ®é—®é¢˜æ‰¾åˆ°æœ€ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µ
    # é»˜è®¤ä½¿ç”¨ç›¸ä¼¼åº¦æœç´¢ï¼Œè¿”å› Top-K ä¸ªæœ€ç›¸ä¼¼çš„æ–‡æ¡£å—
    retriever = vectordb.as_retriever()
    # å¯ä»¥è®¾ç½®æ£€ç´¢å‚æ•°ï¼Œä¾‹å¦‚ï¼š
    # retriever = vectordb.as_retriever(search_kwargs={"k": 4})  # è¿”å› Top-4

    # ========== åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹ ==========
    # LLM è´Ÿè´£åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
    # ä½¿ç”¨æœ¬åœ°éƒ¨ç½²çš„æ¨¡å‹ï¼ˆé€šè¿‡ Ollamaï¼‰ï¼Œä¿æŠ¤æ•°æ®éšç§
    llm = Ollama(model=model_name)

    # ========== æ„å»ºæç¤ºè¯æ¨¡æ¿ ==========
    # å®šä¹‰å¦‚ä½•å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£å’Œç”¨æˆ·é—®é¢˜ç»„åˆæˆæç¤ºè¯
    # {context}: æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µï¼ˆç”±æ£€ç´¢å™¨è‡ªåŠ¨å¡«å……ï¼‰
    # {question}: ç”¨æˆ·çš„é—®é¢˜ï¼ˆç”±ç”¨æˆ·è¾“å…¥ï¼‰
    prompt_template = PromptTemplate(
        template=(
            "å‚è€ƒä»¥ä¸‹æ£€ç´¢åˆ°çš„å†…å®¹å›ç­”é—®é¢˜ã€‚\n\n"
            "æ£€ç´¢å†…å®¹ï¼š\n{context}\n\n"
            "é—®é¢˜ï¼š{question}\n\n"
            "è¯·ç»™å‡ºç®€æ´ç­”æ¡ˆã€‚"
        ),
        input_variables=["context", "question"]
    )

    # ========== ç»„è£…æ£€ç´¢å¢å¼ºç”Ÿæˆé“¾ ==========
    # RetrievalQA é“¾å°†æ£€ç´¢å’Œç”Ÿæˆè¿‡ç¨‹ä¸²è”èµ·æ¥ï¼š
    # 1. æ¥æ”¶ç”¨æˆ·é—®é¢˜
    # 2. ä½¿ç”¨æ£€ç´¢å™¨æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
    # 3. å°†æ–‡æ¡£ç‰‡æ®µå’Œé—®é¢˜ç»„åˆæˆæç¤ºè¯
    # 4. è°ƒç”¨ LLM ç”Ÿæˆç­”æ¡ˆ
    # 5. è¿”å›ç­”æ¡ˆå’Œæºæ–‡æ¡£å¼•ç”¨
    chain = RetrievalQA.from_chain_type(
        llm=llm,                    # ç”Ÿæˆæ¨¡å‹
        retriever=retriever,        # æ£€ç´¢å™¨
        return_source_documents=True,  # è¿”å›æºæ–‡æ¡£ï¼Œä¾¿äºè¿½æº¯ç­”æ¡ˆæ¥æº
        chain_type_kwargs={"prompt": prompt_template}  # è‡ªå®šä¹‰æç¤ºè¯æ¨¡æ¿
    )
    # è¿”å› chain å’Œ prompt_templateï¼Œä¾¿äºæ—¥å¿—è®°å½•
    return chain, prompt_template, retriever


if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    model_name = "qwen2.5:1.5b"  # é»˜è®¤ä½¿ç”¨ 1.5B ç‰ˆæœ¬
    
    if "--model" in sys.argv:
        idx = sys.argv.index("--model")
        if idx + 1 < len(sys.argv):
            model_name = sys.argv[idx + 1]
        else:
            print("âŒ --model å‚æ•°éœ€è¦æŒ‡å®šæ¨¡å‹åç§°")
            print("   ä¾‹å¦‚ï¼špython app.py --model qwen2.5:1.5b")
            exit(1)
    
    if "--check-resources" in sys.argv:
        check_system_resources()

    # æ£€æŸ¥ Ollama æœåŠ¡æ˜¯å¦è¿è¡Œ
    is_connected, error_msg = check_ollama_connection()
    if not is_connected:
        print("âŒ Ollama æœåŠ¡æœªè¿è¡Œï¼")
        print(f"é”™è¯¯ä¿¡æ¯ï¼š{error_msg}\n")
        print("ğŸ“‹ è§£å†³æ–¹æ¡ˆï¼š")
        print("1. å¯åŠ¨ Ollama æœåŠ¡ï¼š")
        print("   - åœ¨ç»ˆç«¯è¿è¡Œï¼šollama serve")
        print("   - æˆ–è€…ç›´æ¥è¿è¡Œï¼šollama run qwen2.5:1.5b")
        print("\n2. ç¡®ä¿å·²å®‰è£… Ollamaï¼š")
        print("   - è®¿é—® https://ollama.ai ä¸‹è½½å®‰è£…")
        print("   - macOS: brew install ollama")
        exit(1)

    # ========== åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ ==========
    conv_logger, debug_logger = setup_logging()
    conv_logger.info("="*80)
    conv_logger.info(f"RAG ç³»ç»Ÿå¯åŠ¨ - æ¨¡å‹: {model_name}")
    debug_logger.debug(f"ç³»ç»Ÿå¯åŠ¨ - æ¨¡å‹: {model_name}")

    # ========== RAG ç³»ç»Ÿåˆå§‹åŒ– ==========
    # æ£€æŸ¥å‘é‡åº“æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æ„å»ºï¼ˆé¦–æ¬¡è¿è¡Œï¼‰
    if not os.path.exists("vectordb"):
        print("ğŸ”§ ç¬¬ä¸€æ¬¡è¿è¡Œï¼Œå¼€å§‹æ„å»ºå‘é‡åº“...")
        print("   è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        build_vectorstore()
        print("\nâœ… å‘é‡åº“æ„å»ºå®Œæˆï¼")

    # åŠ è½½é—®ç­”é“¾ï¼ˆåŒ…å«æ£€ç´¢å™¨å’Œç”Ÿæˆå™¨ï¼‰
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ï¼š{model_name}")
    chain, prompt_template, retriever = load_qa_chain(model_name=model_name)

    print("ğŸŸ¢ Mini-RAG æ–‡æ¡£é—®ç­”ç³»ç»Ÿå·²å¯åŠ¨ï¼ˆè¾“å…¥ exit é€€å‡ºï¼‰")
    print("\n" + "="*60)
    print("ğŸ’¡ RAG å·¥ä½œæµç¨‹ï¼š")
    print("   1. æ‚¨è¾“å…¥é—®é¢˜")
    print("   2. ç³»ç»Ÿåœ¨æ–‡æ¡£ä¸­æ£€ç´¢ç›¸å…³å†…å®¹")
    print("   3. å°†ç›¸å…³å†…å®¹ä¸é—®é¢˜ä¸€èµ·å‘é€ç»™ AI")
    print("   4. AI åŸºäºæ–‡æ¡£å†…å®¹ç”Ÿæˆç­”æ¡ˆ")
    print("="*60 + "\n")
    
    # ========== äº¤äº’å¼é—®ç­”å¾ªç¯ ==========
    while True:
        query = input("\nè¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š")
        if query.lower() == "exit":
            print("\nğŸ‘‹ å†è§ï¼")
            break

        try:
            # ========== è®°å½•ç”¨æˆ·é—®é¢˜ ==========
            conv_logger.info(f"\n{'='*80}")
            conv_logger.info(f"ã€ç”¨æˆ·é—®é¢˜ã€‘{query}")
            debug_logger.debug(f"ç”¨æˆ·é—®é¢˜: {query}")
            
            # ========== RAG æ ¸å¿ƒæµç¨‹ï¼šæ£€ç´¢ + ç”Ÿæˆ ==========
            # chain.invoke() ä¼šè‡ªåŠ¨æ‰§è¡Œä»¥ä¸‹æ­¥éª¤ï¼š
            # 
            # ã€æ­¥éª¤1ï¼šé—®é¢˜å‘é‡åŒ–ã€‘
            #   å°†ç”¨æˆ·é—®é¢˜è½¬æ¢ä¸ºå‘é‡ï¼ˆä½¿ç”¨åµŒå…¥æ¨¡å‹ï¼‰
            #
            # ã€æ­¥éª¤2ï¼šç›¸ä¼¼åº¦æ£€ç´¢ã€‘
            #   åœ¨å‘é‡æ•°æ®åº“ä¸­æœç´¢ä¸é—®é¢˜å‘é‡æœ€ç›¸ä¼¼çš„æ–‡æ¡£ç‰‡æ®µ
            #   é»˜è®¤è¿”å› Top-K ä¸ªæœ€ç›¸å…³çš„æ–‡æ¡£å—
            #
            # ã€æ­¥éª¤3ï¼šä¸Šä¸‹æ–‡å¢å¼ºã€‘
            #   å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µä½œä¸ºä¸Šä¸‹æ–‡ï¼Œä¸é—®é¢˜ç»„åˆï¼š
            #   "å‚è€ƒä»¥ä¸‹æ£€ç´¢åˆ°çš„å†…å®¹å›ç­”é—®é¢˜ã€‚
            #    æ£€ç´¢å†…å®¹ï¼š[æ–‡æ¡£ç‰‡æ®µ1] [æ–‡æ¡£ç‰‡æ®µ2] ...
            #    é—®é¢˜ï¼š[ç”¨æˆ·é—®é¢˜]
            #    è¯·ç»™å‡ºç®€æ´ç­”æ¡ˆã€‚"
            #
            # ã€æ­¥éª¤4ï¼šç”Ÿæˆç­”æ¡ˆã€‘
            #   å°†å¢å¼ºåçš„æç¤ºè¯å‘é€ç»™ LLM
            #   LLM åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
            #
            # ã€æ­¥éª¤5ï¼šè¿”å›ç»“æœã€‘
            #   result["result"]: ç”Ÿæˆçš„ç­”æ¡ˆ
            #   result["source_documents"]: æ£€ç´¢åˆ°çš„æºæ–‡æ¡£ç‰‡æ®µ
            
            # ========== æ­¥éª¤2ï¼šæ‰‹åŠ¨æ‰§è¡Œæ£€ç´¢ï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰==========
            # å…ˆæ£€ç´¢æ–‡æ¡£ç‰‡æ®µï¼Œè®°å½•åˆ°æ—¥å¿—
            retrieved_docs = retriever.get_relevant_documents(query)
            conv_logger.info(f"\nã€æ£€ç´¢åˆ°çš„æ–‡æ¡£ç‰‡æ®µæ•°é‡ã€‘{len(retrieved_docs)}")
            debug_logger.debug(f"æ£€ç´¢åˆ° {len(retrieved_docs)} ä¸ªæ–‡æ¡£ç‰‡æ®µ")
            
            # è®°å½•æ£€ç´¢åˆ°çš„æ–‡æ¡£å†…å®¹
            for i, doc in enumerate(retrieved_docs, 1):
                doc_content = doc.page_content[:500]  # å‰500å­—ç¬¦
                conv_logger.info(f"\nã€æ£€ç´¢ç‰‡æ®µ {i}/{len(retrieved_docs)}ã€‘")
                conv_logger.info(f"{doc_content}...")
                debug_logger.debug(f"æ£€ç´¢ç‰‡æ®µ {i}: {doc_content[:200]}...")
            
            # ========== æ­¥éª¤3ï¼šæ„å»ºæœ€ç»ˆçš„ Prompt ==========
            # å°†æ£€ç´¢åˆ°çš„æ–‡æ¡£ç»„åˆæˆä¸Šä¸‹æ–‡
            context = "\n\n".join([doc.page_content for doc in retrieved_docs])
            
            # ä½¿ç”¨ prompt_template æ ¼å¼åŒ–æœ€ç»ˆçš„ prompt
            final_prompt = prompt_template.format(
                context=context,
                question=query
            )
            
            # ========== è®°å½•æœ€ç»ˆçš„ Prompt ==========
            conv_logger.info("\nã€æœ€ç»ˆ Promptã€‘")
            conv_logger.info(final_prompt)
            debug_logger.debug(f"æœ€ç»ˆ Prompt:\n{final_prompt}")
            
            # ========== æ­¥éª¤4ï¼šè°ƒç”¨é“¾ç”Ÿæˆç­”æ¡ˆ ==========
            result = chain.invoke({"query": query})
            
            # ========== è®°å½•æœ€ç»ˆå“åº” ==========
            answer = result["result"]
            conv_logger.info("\nã€AI å›ç­”ã€‘")
            conv_logger.info(answer)
            conv_logger.info(f"\n{'='*80}\n")
            debug_logger.debug(f"AI å›ç­”: {answer}")
            
            # æ˜¾ç¤ºç­”æ¡ˆ
            print("\nğŸ“˜ ç­”æ¡ˆï¼š\n", answer)

            # å¯é€‰ï¼šæ˜¾ç¤ºæ£€ç´¢åˆ°çš„æºæ–‡æ¡£ç‰‡æ®µï¼ˆå·²æ³¨é‡Šï¼‰
            # å–æ¶ˆæ³¨é‡Šå¯ä»¥æŸ¥çœ‹ç­”æ¡ˆçš„æ¥æºï¼Œä¾¿äºéªŒè¯ç­”æ¡ˆçš„å‡†ç¡®æ€§
            # print("\nğŸ“š å¼•ç”¨ç‰‡æ®µï¼š")
            # for doc in result["source_documents"]:
            #     print("-", doc.page_content[:200], "...")
        except requests.exceptions.ConnectionError:
            print("\nâŒ è¿æ¥ Ollama å¤±è´¥ï¼Œè¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œ")
            print("   è¿è¡Œå‘½ä»¤ï¼šollama serve")
            break
        except Exception as e:
            print(f"\nâŒ å‘ç”Ÿé”™è¯¯ï¼š{str(e)}")
            break
